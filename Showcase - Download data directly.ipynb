{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b954e8-757a-4552-9a2c-712c7272468a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import pandas as pd\n",
    "import notebookutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0b6e3-1eea-4d7b-adf1-ffc1d98bad3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "tenant_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23aa1e-6283-4864-823a-2f84a96f2683",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "tables = [\"customerrecords\",\"productrecords\",\"salesrecords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50900b01-94c9-405e-a7c0-0b651e261b8d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "keyvault_name = \"\"\n",
    "keyvault_ir = f'https://{keyvault_name}.vault.azure.net/'\n",
    "dataverse_prefix = \"\" # Example: cr8dc\n",
    "\n",
    "client_id = notebookutils.credentials.getSecret(keyvault_ir,'dataverseFabricIntegrationClientId')\n",
    "client_secret = notebookutils.credentials.getSecret(keyvault_ir,'dataverseFabricIntegrationClientSecret')\n",
    "dataverse_name = \"\" # Example: org9q35785d\n",
    "resource = f'https://{dataverse_name}.crm4.dynamics.com'\n",
    "\n",
    "authority = f'https://login.microsoftonline.com/{tenant_id}'\n",
    "\n",
    "\n",
    "entity_prefix = f'{dataverse_prefix}_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bac2d-785e-42fc-8dc2-9300c4d84473",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def get_access_token():\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        client_id,\n",
    "        authority=authority,\n",
    "        client_credential=client_secret\n",
    "    )\n",
    "    token_response = app.acquire_token_for_client(scopes=[f\"{resource}/.default\"])\n",
    "    return token_response.get(\"access_token\")\n",
    "\n",
    "def query_dataverse(api_url, access_token):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"OData-MaxVersion\": \"4.0\",\n",
    "        \"OData-Version\": \"4.0\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json().get('value', [])\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "def select_and_rename_columns(df, prefix):\n",
    "    selected_columns = [col for col in df.columns if col.startswith(prefix)]\n",
    "    renamed_columns = {col: col[len(prefix):] for col in selected_columns}\n",
    "    new_df = df[selected_columns].rename(columns=renamed_columns)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339675d-5a34-460f-aace-611f55b6d21e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Demo Parameters\n",
    "entity_name = entity_prefix+'salesrecords'\n",
    "entity_url = f'{resource}/api/data/v9.2/{entity_name}'\n",
    "\n",
    "# Query Dataverse\n",
    "access_token = get_access_token()\n",
    "result_df = query_dataverse(entity_url, access_token)\n",
    "\n",
    "# Display Results\n",
    "if result_df is not None:\n",
    "    result_df = select_and_rename_columns(result_df,entity_prefix)\n",
    "    display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64fac6-85dc-4a85-8acc-715ca998a621",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Vytvoření trídy MountedWriter pro zápis do OneLake\n",
    "class MountedWriter:\n",
    "    def __init__(self, workspace_id, lakehouse_id, parent_folder_name):\n",
    "        self.workspace = workspace_id\n",
    "        self.lakehouse = lakehouse_id\n",
    "        self.parent_folder_name = parent_folder_name\n",
    "        self.mount_name = \"/mnt/lakehouse\"\n",
    "        self.mount = notebookutils.fs.mount(\n",
    "            f\"abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse}\",\n",
    "            self.mount_name\n",
    "        )\n",
    "\n",
    "    def get_mounted_path(self):\n",
    "        self.mount_path = notebookutils.fs.getMountPath(self.mount_name)\n",
    "        return self.mount_path\n",
    "\n",
    "    def check_or_create_existing_directory(self, folder_name):\n",
    "        self.mount_path = self.get_mounted_path()\n",
    "        output_dir = f\"{self.mount_path}/Files/{self.parent_folder_name}/{folder_name}\"\n",
    "        notebookutils.fs.mkdirs(output_dir)\n",
    "\n",
    "    def create_file(self, df_to_be_written, folder_name, file_name_parquet):\n",
    "        self.check_or_create_existing_directory(folder_name)\n",
    "        output_dir = f\"{self.mount_path}/Files/{self.parent_folder_name}/{folder_name}\"\n",
    "        output_file = f\"{output_dir}/{file_name_parquet}\"\n",
    "        df_to_be_written.to_parquet(output_file)\n",
    "\n",
    "    def end_mounting(self):\n",
    "        notebookutils.fs.unmount(self.mount_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252c602-3af9-48ac-9f82-bca506987c2b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspace_id = \"\"\n",
    "lakehouse_id = \"\"\n",
    "\n",
    "parent_folder_name = \"Dataverse\"\n",
    "folder_name = \"Sales\"\n",
    "# Will create folder structure: /mnt/lakehouse/Files/Dataverse/Sales\n",
    "\n",
    "file_name = \"export\"\n",
    "file_name_parquet = f\"{file_name}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca5cf1-3e18-497c-ba3a-b8de44047c02",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "mount_writer = MountedWriter(\n",
    "    workspace_id = workspace_id, \n",
    "    lakehouse_id = lakehouse_id, \n",
    "    parent_folder_name = parent_folder_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279916e3-2583-49e3-8eb8-3171a4cb642b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "mount_writer.create_file(df_to_be_written = result_df, folder_name = folder_name, file_name_parquet = file_name_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df77f3-b252-4267-94e4-7008a067e49d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    entity_name = entity_prefix+table\n",
    "    entity_url = f'{resource}/api/data/v9.2/{entity_name}'\n",
    "\n",
    "    print(f\"Currently handling table: {table}\")\n",
    "\n",
    "    # Query Dataverse\n",
    "    access_token = get_access_token()\n",
    "    result_df = query_dataverse(entity_url, access_token)\n",
    "\n",
    "    # Wrtite Results\n",
    "    if result_df is not None:\n",
    "        result_df = select_and_rename_columns(result_df,entity_prefix)\n",
    "        mount_writer.create_file(df_to_be_written = result_df, folder_name = table, file_name_parquet = file_name_parquet)\n",
    "        print(f\"Data from table '{table}' has been written do Lakehouse '{lakehouse_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120a5da7-e68c-467a-8428-f179d7e25430",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "notebookutils.notebook.exit(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
